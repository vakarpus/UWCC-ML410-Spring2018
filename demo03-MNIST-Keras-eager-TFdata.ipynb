{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-B5UZxUchfd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST with Eager Execution.\n",
    "\n",
    "* MNIST data converted to `tf.data` [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "* This enables convenient data manipulation: parallel reads, prefetching, batching, shuffling, etc\n",
    "* Learn more about `tf.data`: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "Code is similar to the previous notebook with a few modifications:\n",
    "\n",
    "1. Instead of using `model.fit`, we use `model.train_on_batch` to demonstrate batch iteration over the dataset. \n",
    "2. Using TensorFlow optimizer to pass to the Keras model.\n",
    "3. Using enable eager execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eager execution\n",
    "Eager execution is a mode for running TensorFlow that works just like regular Python. https://www.tensorflow.org/programmers_guide/eager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "902Rjd5DZroO",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 17 22:13:20 UTC 2018\n",
      "smallDLVM1\n",
      "Collecting tensorflow==1.8.0\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/dc/464f59597a5a8282585238e6e3a7bb3770c3c1f1dc8ee72bd5be257178ec/tensorflow-1.8.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.4.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/c3/9b947e301e19bea75dc8c1fd3710eed5d2b31aa13ae13d5e38e891f784cc/protobuf-3.5.2.post1-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl\n",
      "Collecting wheel>=0.26 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/30/e935244ca6165187ae8be876b6316ae201b71485538ffac1d718843025a9/wheel-0.31.1-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.8.0)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/ff/f118147fd7a8d2d441d15e1cb7fefb2c1981586e24ef3a7d8a742535b085/grpcio-1.12.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.8.0)\n",
      "Collecting numpy>=1.13.3 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/61/11b05cc37ccdaabad89f04dbdc2a02905cf6de6f9b05816dba843beed328/numpy-1.14.3-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.8.0)\n",
      "Collecting setuptools (from protobuf>=3.4.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\n",
      "\u001b[31mazure-cosmosdb-table 1.0.1 has requirement azure-storage-common<0.38.0,>=0.37.1, but you'll have azure-storage-common 1.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-advisor~=1.0, but you'll have azure-mgmt-advisor 0.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-batch~=5.0, but you'll have azure-mgmt-batch 4.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-cdn~=2.0, but you'll have azure-mgmt-cdn 0.30.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-cognitiveservices~=2.0, but you'll have azure-mgmt-cognitiveservices 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-consumption~=2.0, but you'll have azure-mgmt-consumption 0.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-containerinstance~=0.3.1, but you'll have azure-mgmt-containerinstance 0.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-containerservice~=3.0, but you'll have azure-mgmt-containerservice 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-cosmosdb~=0.3.1, but you'll have azure-mgmt-cosmosdb 0.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-datalake-analytics~=0.3.0, but you'll have azure-mgmt-datalake-analytics 0.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-datalake-store~=0.3.0, but you'll have azure-mgmt-datalake-store 0.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-devtestlabs~=2.1, but you'll have azure-mgmt-devtestlabs 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-eventgrid~=0.4.0, but you'll have azure-mgmt-eventgrid 0.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-monitor~=0.4.0, but you'll have azure-mgmt-monitor 0.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-network~=1.7, but you'll have azure-mgmt-network 1.5.0rc3 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-recoveryservices~=0.2.0, but you'll have azure-mgmt-recoveryservices 0.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-redis~=5.0, but you'll have azure-mgmt-redis 4.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-sql~=0.8.5, but you'll have azure-mgmt-sql 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-mgmt 2.0.0 has requirement azure-mgmt-storage~=1.5, but you'll have azure-mgmt-storage 1.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure 3.0.0 has requirement azure-datalake-store~=0.0.18, but you'll have azure-datalake-store 0.0.17 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure 3.0.0 has requirement azure-graphrbac~=0.40.0, but you'll have azure-graphrbac 0.31.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure 3.0.0 has requirement azure-servicefabric~=6.1.2.9, but you'll have azure-servicefabric 5.6.130 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-batch-extensions 1.0.1 has requirement azure-storage<0.35,>=0.32, but you'll have azure-storage 0.36.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: astor, six, setuptools, protobuf, html5lib, bleach, wheel, markdown, numpy, werkzeug, tensorboard, gast, grpcio, termcolor, absl-py, tensorflow\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 1] Operation not permitted\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!hostname\n",
    "!pip install --ignore-installed --upgrade tensorflow==1.8.0\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable eager execution\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "evWlYUkYefG8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JA3braIeeggc"
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = len(train_images)\n",
    "TEST_SIZE = len(test_images)\n",
    "\n",
    "# Reshape from (N, 28, 28) to (N, 784)\n",
    "train_images = np.reshape(train_images, (TRAINING_SIZE, 784))\n",
    "test_images = np.reshape(test_images, (TEST_SIZE, 784))\n",
    "\n",
    "# Convert the array to float32 as opposed to uint8\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "\n",
    "# Convert the pixel values from integers between 0 and 255 to floats between 0 and 1\n",
    "train_images /= 255\n",
    "test_images /=  255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wbwRqi0BeicT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 5\n",
      "After [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "NUM_DIGITS = 10\n",
    "\n",
    "print(\"Before\", train_labels[0]) # The format of the labels before conversion\n",
    "\n",
    "train_labels  = tf.keras.utils.to_categorical(train_labels, NUM_DIGITS)\n",
    "\n",
    "print(\"After\", train_labels[0]) # The format of the labels after conversion\n",
    "\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, NUM_DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XFtvnaI5jU_5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Cast the labels to floats, needed later\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jjNr3gr3ejh2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "\n",
    "# Create a TensorFlow optimizer, rather than using the Keras version\n",
    "# This is currently necessary when working in eager mode\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "# We will now compile and print out a summary of our model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUZPvDQYe5Xu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1) Create a tf.data Dataset\n",
    "\n",
    "Here, we'll use the `tf.data.Dataset` [API](https://www.tensorflow.org/api_docs/python/tf/data) to convert the Numpy arrays into a TensorFlow dataset.\n",
    "\n",
    "We will create a simple for loop that will serve as our introduction into creating custom training loops. Although this essentially does the same thing as `model.fit` it allows us to customize the overall training process and collect different metrics throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sdBd2pd_fdue"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "\n",
    "# Because tf.data may work with potentially **large** collections of data\n",
    "# we do not shuffle the entire dataset by default\n",
    "# Instead, we maintain a buffer of SHUFFLE_SIZE elements\n",
    "# and sample from there.\n",
    "SHUFFLE_SIZE = 10000 \n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "dataset = dataset.shuffle(SHUFFLE_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksAR-C6xgUu4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2) Iterate over the dataset\n",
    "Here, we'll iterate over the dataset, and train our model using `model.train_on_batch`. To learn more about the elements returned from the dataset, you can print them out and try the `.numpy()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kNgnUKPvgSCz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\t Loss: 0.163038\tAccuracy: 0.927083\n",
      "Epoch #2\t Loss: 0.140783\tAccuracy: 0.958333\n",
      "Epoch #3\t Loss: 0.013712\tAccuracy: 0.989583\n",
      "Epoch #4\t Loss: 0.048416\tAccuracy: 0.989583\n",
      "Epoch #5\t Loss: 0.009129\tAccuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for images, labels in dataset:\n",
    "    train_loss, train_accuracy = model.train_on_batch(images, labels)\n",
    "  \n",
    "  # Here you can gather any metrics or adjust your training parameters\n",
    "  print('Epoch #%d\\t Loss: %.6f\\tAccuracy: %.6f' % (epoch + 1, train_loss, train_accuracy))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tg5U3Iqkgo3J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 88us/step\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fu0eSNa9kEFZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Congratulations\n",
    "You have trained a model on MNIST using Keras, eager execution, and tf.data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "2-mnist-with-keras-eager-and-tf-data.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
