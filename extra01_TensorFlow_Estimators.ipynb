{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ```tf.estimator.Estimator``` class\n",
    "Estimators encapsulate the following actions:\n",
    "\n",
    "* training\n",
    "* evaluation\n",
    "* prediction\n",
    "* export for serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benefits of Estimators\n",
    "* Estimators are abstracted out from single or multi-node setup on CPUs, GPUs, TPUs (and FPGAs in the future)\n",
    "* Simplify sharing your work\n",
    "* Estimators are build on top of ```ft.layer```, thus customization is simple\n",
    "* You don't have to build the graph, it is build for you\n",
    "\n",
    "But you need:\n",
    "* Separate data input pipline and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pre-made Estimators create and manage ```Graph``` and ```Session``` objects for you.\n",
    "\n",
    "## Structure of a pre-made Estimators program\n",
    "1. Write one or more dataset importing functions\n",
    "2. Define the feature columns\n",
    "3. Instantiate the relevant pre-made Estimator\n",
    "4. Call a training, evaluation, or inference method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## 1. Write one or more dataset importing functions\n",
    "    \n",
    "Each dataset importing function must return two objects:\n",
    "* a dictionary in which the keys are feature names and the values are feature Tensors\n",
    "* labels tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def input_fn(dataset):\n",
    "    feature_dict = {}\n",
    "    label = []\n",
    "    # ...  manipulate dataset, extracting feature names and the label\n",
    "    return feature_dict, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "  features = data[\"features\"]\n",
    "  labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* embed the ```features``` and ```labels``` arrays in the TensorFlow graph as ```tf.constant()``` operations\n",
    "* works well for a small datase\n",
    "* data from ```features``` and ```labels``` replicated twice in memory\n",
    "* Careful!!! Can run into the 2GB limit for the ```tf.GraphDef``` protocol buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively,\n",
    "* define the ```Dataset``` in terms of ```tf.placeholder()``` tensors\n",
    "* feed the NumPy arrays during initialization of ```Iterator``` over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "  features = data[\"features\"]\n",
    "  labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "# [Other transformations on `dataset`...]\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoding image data and resizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "  image_string = tf.read_file(filename)\n",
    "  image_decoded = tf.image.decode_image(image_string)\n",
    "  image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "  return image_resized, label\n",
    "\n",
    "# A vector of filenames.\n",
    "filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i].\n",
    "labels = tf.constant([0, 37, ...])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simple batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Define the feature columns\n",
    "Each ```tf.feature_column``` identifies a feature name, its type, and any input pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define three numeric feature columns.\n",
    "population = tf.feature_column.numeric_column('population')\n",
    "crime_rate = tf.feature_column.numeric_column('crime_rate')\n",
    "median_education = tf.feature_column.numeric_column('median_education',\n",
    "                    normalizer_fn='lambda x: x - global_education_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Instantiate the relevant pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an estimator, passing the feature columns.\n",
    "estimator = tf.estimator.Estimator.LinearClassifier(\n",
    "    feature_columns=[population, crime_rate, median_education],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4. Call a training, evaluation, or inference method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_training_set is the function created in Step 1\n",
    "estimator.train(input_fn=my_training_set, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a Keras inception v3 model.\n",
    "keras_inception_v3 = tf.keras.applications.inception_v3.InceptionV3(weights=None)\n",
    "# Compile model with the optimizer, loss, and metrics you'd like to train with.\n",
    "keras_inception_v3.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metric='accuracy')\n",
    "# Create an Estimator from the compiled Keras model. Note the initial model\n",
    "# state of the keras model is preserved in the created Estimator.\n",
    "est_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v3)\n",
    "\n",
    "# Treat the derived Estimator as you would with any other Estimator.\n",
    "# First, recover the input name(s) of Keras model, so we can use them as the\n",
    "# feature column name(s) of the Estimator input function:\n",
    "keras_inception_v3.input_names  # print out: ['input_1']\n",
    "# Once we have the input name(s), we can create the input function, for example,\n",
    "# for input(s) in the format of numpy ndarray:\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input_1\": train_data},\n",
    "    y=train_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "# To train, we call Estimator's train function:\n",
    "est_inception_v3.train(input_fn=train_input_fn, steps=2000)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
