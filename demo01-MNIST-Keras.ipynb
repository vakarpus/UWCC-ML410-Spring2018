{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZrAitlFLdEZ"
   },
   "source": [
    "# MNIST Dataset (Handwritten digist) with tf.keras\n",
    "\n",
    "On this notebook you'll learn how to train an image classifier on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) - the \"hello world\" of computer vision: loading the data, building and training a model, calculating the accuracy, and making predictions. Let's proceed with checking the TensorFlow version (should be 1.8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jSmUsjJfMEqC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/dc/464f59597a5a8282585238e6e3a7bb3770c3c1f1dc8ee72bd5be257178ec/tensorflow-1.8.0-cp35-cp35m-manylinux1_x86_64.whl (49.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 49.1MB 30kB/s  eta 0:00:01    95% |██████████████████████████████▋ | 47.0MB 68.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.9.0,>=1.8.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl\n",
      "Collecting numpy>=1.13.3 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/61/11b05cc37ccdaabad89f04dbdc2a02905cf6de6f9b05816dba843beed328/numpy-1.14.3-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting protobuf>=3.4.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/c3/9b947e301e19bea75dc8c1fd3710eed5d2b31aa13ae13d5e38e891f784cc/protobuf-3.5.2.post1-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting wheel>=0.26 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/30/e935244ca6165187ae8be876b6316ae201b71485538ffac1d718843025a9/wheel-0.31.1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/69/6c86a51c6cf5ad6f09a54c9a7aae09850166af8d99cf1418e9ea9250baca/absl-py-0.2.1.tar.gz (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 12.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/ff/f118147fd7a8d2d441d15e1cb7fefb2c1981586e24ef3a7d8a742535b085/grpcio-1.12.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 10.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 4.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from protobuf>=3.4.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: termcolor, absl-py, gast, html5lib\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/vadim/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/vadim/.cache/pip/wheels/84/69/e0/6f4b789daf6cae7e70e5fda095603c7746cdc1a6013303c7ff\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/vadim/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/vadim/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built termcolor absl-py gast html5lib\n",
      "Installing collected packages: wheel, markdown, six, html5lib, setuptools, protobuf, numpy, bleach, werkzeug, tensorboard, astor, termcolor, absl-py, gast, grpcio, tensorflow\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/pip/wheel.py\", line 329, in clobber\n",
      "    os.utime(destfile, (st.st_atime, st.st_mtime))\n",
      "PermissionError: [Errno 1] Operation not permitted\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed --upgrade tensorflow==1.8.0\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "# or\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8Lhscw0NDln"
   },
   "source": [
    "### Step 1. Downloading the dataset\n",
    "\n",
    "The MNIST dataset contains 60'000 grayscale images in training dataset and 10'000 grayscale images in testing dataset of handwritten digits. They will be downloaded automatically using ```tf.keras.datasets.mnist``` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FKiwTuT-NE6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEFU58MaNPpk"
   },
   "source": [
    "### Step 2. Visualizing the data\n",
    "Let's look at the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AwxNOsCMNNGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60a3dc2be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "i = random.randint(0, 100)\n",
    "\n",
    "print(\"Label: %s\" % train_labels[i])\n",
    "plt.imshow(train_images[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2n2NVdKNk5i"
   },
   "source": [
    "### Step 3. Understanding the data layout\n",
    "\n",
    "We are given dataset as a list of 2D grayscale 28 by 28 images. So, it is a 3-D array of integer values that is of shape (*N*, 28, 28), where *N* is the number of images in the training or test set. The labels are 1-D array of the integer values of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TTj2ZWMBN24i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  41 152 233 254 213  82   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 123 243 253 252 253 252 243  81   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  21 132 253 254 213 142  61  31 233 254 131   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  21 203 253 212  50  10   0  41 132 252 172  10   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 153 253 163   0   0   0   0 102 254 253 102   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 193 252   0   0   0   0   0 142 253 252  20   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 254 151   0   0   0   0 132 253 254 233   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 253 232 183 102 102 183 253 252 253 111   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 203 243 254 253 254 213 152 253 224  20   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  40 151 151  91  10 152 252 162   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 163 254 192   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 203 253  70   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 132 253 142   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  41 253 252  61   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  72 253 254  91   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 193 252  91  10   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 123 255 192   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  82 243 233  50   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 153 253 183   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 152 212  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "#np.set_printoptions(precision=2)\n",
    "np.set_printoptions(linewidth=1000)\n",
    "print(train_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eo_cZXaqODnZ"
   },
   "source": [
    "### Step 4. Reformating the images\n",
    "\n",
    "For our model (and for our own convenience) let's modify the layout of our data from 28*28 2D representation to 1D 784 (= 28 x 28) array because we plan to use fully-connected neural network as our model and convert ```integer``` grayscale intensity to ```float``` format by dividing element-wise everything my 255 (the highest intensity) - rescaling these intensities from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OgnV5FJjP5Vz"
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = len(train_images)\n",
    "TEST_SIZE = len(test_images)\n",
    "\n",
    "# Reshape from (N, 28, 28) to (N, 784)\n",
    "train_images = np.reshape(train_images, (TRAINING_SIZE, 784))\n",
    "test_images = np.reshape(test_images, (TEST_SIZE, 784))\n",
    "\n",
    "# Convert the array to float32 as opposed to uint8\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "\n",
    "# Convert the pixel values from integers between 0 and 255 to floats between 0 and 1\n",
    "train_images /= 255\n",
    "test_images /=  255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the layout again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.6 0.9 1.  0.8 0.3 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 1.  1.  1.  1.  1.  1.  0.3 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.5 1.  1.  0.8 0.6 0.2 0.1 0.9 1.  0.5 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.8 1.  0.8 0.2 0.  0.  0.2 0.5 1.  0.7 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 1.  0.6 0.  0.  0.  0.  0.4 1.  1.  0.4 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 1.  0.  0.  0.  0.  0.  0.6 1.  1.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.6 0.  0.  0.  0.  0.5 1.  1.  0.9 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.9 0.7 0.4 0.4 0.7 1.  1.  1.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 1.  1.  1.  1.  0.8 0.6 1.  0.9 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.6 0.6 0.4 0.  0.6 1.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 1.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 1.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 1.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 1.  1.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 1.  1.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 1.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 1.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 1.  0.9 0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 1.  0.7 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.8 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "np.set_printoptions(precision=1)\n",
    "np.set_printoptions(linewidth=4*29)\n",
    "print(train_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GI25z0StQH-P"
   },
   "source": [
    "### Step 5. Reformating the labels\n",
    "\n",
    "Next, we want to convert the labels from an integer format (e.g., \"2\"), to a [one hot encoding](https://en.wikipedia.org/wiki/One-hot) (e.g., \"0, 0, 1, 0, 0, 0, 0, 0, 0, 0\"). To do so, we'll use the ```tf.keras.utils.to_categorical``` [function](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "E9yrkEENQ9Vz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 9\n",
      "After [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "NUM_DIGITS = 10\n",
    "\n",
    "print(\"Before\", train_labels[i]) # The format of the labels before conversion\n",
    "\n",
    "train_labels  = tf.keras.utils.to_categorical(train_labels, NUM_DIGITS)\n",
    "\n",
    "print(\"After\", train_labels[i]) # The format of the labels after conversion\n",
    "\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, NUM_DIGITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjdbemHURkpv"
   },
   "source": [
    "### Step 6. Building the model\n",
    "\n",
    "Now, we'll create our neural network using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential). \n",
    "* We will use fully-connected one-layer neural network \n",
    "* The hidden layer will have 512 units using the [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation function. \n",
    "* The output layer will have 10 units and use [softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) function. \n",
    "* Notice, we specify the input shape on the first layer. If you add subsequent layers, this is not required. \n",
    "* We will use the [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) loss function, and the [RMSProp](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mNscbvHkUrMc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# We will now compile and print out a summary of our model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Visualizing model with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallDLVM2\r\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "# Don't run on Azure Notebooks\n",
    "#!tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3br9Yi6VuBT"
   },
   "source": [
    "### Step 8. Training the model\n",
    "\n",
    "Next, we will train the model by using the [fit method](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) for 5 [epochs](https://www.quora.com/What-is-epochs-in-machine-learning). We will keep track of the training loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gBs0LwqcVXx6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 17s 283us/step - loss: 0.2025 - acc: 0.9399\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0904 - acc: 0.9735\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0663 - acc: 0.9811\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0500 - acc: 0.9860\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0411 - acc: 0.9889\n",
      "40.941328287124634\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "print(time.time() - start )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcYMPkwkWIPq"
   },
   "source": [
    "### Step 9. Testing\n",
    "Now that we have trained our model, we want to evaluate it. Sure, our model is >97% accurate on the training set, but what about on data it hasn't seen before? The test accuracy is a good metric for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iuqDe4NiWBpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jo-yoMwvXkw6"
   },
   "source": [
    "## Congratulations\n",
    "\n",
    "You have successfully used TensorFlow Keras to train a model on the MNIST dataset.\n",
    "Let's update TensorBoard Graph visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "1-mnist-with-keras.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
