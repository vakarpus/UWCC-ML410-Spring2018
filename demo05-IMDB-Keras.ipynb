{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classifying movie reviews with tf.keras, tf.data, and eager execution.\n",
    "\n",
    "In this lab you'll learn how to classify a movie review as positive or negative on the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb). This tutorial was inspired by [this work](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb). Here, we'll use tf.keras and tf.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 17 21:13:35 UTC 2018\n",
      "nbserver\n",
      "Collecting tensorflow==1.8.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/0e/8af18d9169ed4f1a1c72cd50defd95b8382a12f2cf7cb9b76ba053db79ad/tensorflow-1.8.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting protobuf>=3.4.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/9d/61/54c3a9cfde6ffe0ca6a1786ddb8874263f4ca32e7693ad383bd8cf935015/protobuf-3.5.2.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting wheel (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/30/e935244ca6165187ae8be876b6316ae201b71485538ffac1d718843025a9/wheel-0.31.1-py2.py3-none-any.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting enum34>=1.1.6 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.8.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.8.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.8.0)\n",
      "Collecting six>=1.10.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/4d/1e/3bfb48ff165e331c0c5fdca5de79d497fa5d71f3cb2eee2733ff22e898df/tensorboard-1.8.0-py2-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/2a/d5/eb371e43d65989267a9daa90ee8f92d79b216184ea3c8cf0670e5c2388eb/grpcio-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.13.3 (from tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/e7/08f059a00367fd613e4f2875a16c70b6237268a1d6d166c6d36acada8301/numpy-1.14.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting setuptools (from protobuf>=3.4.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/9d/7bfab757977067556c7ca5fe437f28e8b8843c95564fca504de79df63b25/pbr-4.0.3-py2.py3-none-any.whl\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting futures>=3.1.1; python_version < \"3\" (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/99/b2c4e9d5a30f6471e410a146232b4118e697fa3ffc06d6a65efde84debd0/futures-3.2.0-py2-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0)\n",
      "\u001b[31mfastlmm 0.2.32 requires dill, which is not installed.\u001b[0m\n",
      "\u001b[31mgrin 1.2.1 requires argparse>=1.1, which is not installed.\u001b[0m\n",
      "Installing collected packages: six, setuptools, protobuf, astor, backports.weakref, wheel, funcsigs, pbr, mock, enum34, gast, termcolor, absl-py, numpy, html5lib, bleach, futures, markdown, werkzeug, tensorboard, grpcio, tensorflow\n",
      "Successfully installed absl-py-0.2.1 astor-0.6.2 backports.weakref-1.0.post1 bleach-2.0.0 enum34-1.1.6 funcsigs-1.0.2 futures-3.2.0 gast-0.2.0 grpcio-1.12.0 html5lib-0.999999999 markdown-2.6.11 mock-2.0.0 numpy-1.14.3 pbr-4.0.3 protobuf-3.5.2.post1 setuptools-39.1.0 six-1.11.0 tensorboard-1.8.0 tensorflow-1.8.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.31.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda2_501/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!hostname\n",
    "!pip install --ignore-installed --upgrade tensorflow==1.8.0\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable eager execution\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A first look at the dataset\n",
    "\n",
    "### Step 1) Downloading the dataset\n",
    "\n",
    "The \"IMDB dataset\" is a set of around 50,000 positive or negative reviews for movies from the Internet Movie Database. Let's use the TensorFlow API to download the dataset the same way we did for MNIST. The `num_words` argument is how many of the most popular words we will keep. All other \"rare\" words will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n",
      "17473536/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2) Exploring the dataset \n",
    "\n",
    "The dataset comes preprocessed. Each example is an array of integers representing the words of the movie review. Each label is either a \"0\" for negative or \"1\" for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KmM0e64pok8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Word indexing\n",
    "\n",
    "To prove we have limited ourselves to the 10,000 most frequent words we will iterate over all the reviews and check the maximum value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GvgpnR2ppqHd"
   },
   "outputs": [],
   "source": [
    "print(max([max(review) for review in train_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wJg2FiYpuoX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Converting the integers back to words\n",
    "\n",
    "We can get the dictionary from `tf.keras.datasets.imdb.get_word_index` which hashes the words to their corresponding integers. Let's try and convert a review from integers back into it's original text by first reversing this dictionary and then iterating over a review and converting the integers to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tr5s_1alpzop"
   },
   "outputs": [],
   "source": [
    "# Dictionary that hashes words to their integer\n",
    "word_to_integer = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Print out the first ten keys in the dictionary\n",
    "print(word_to_integer.keys()[0:10])\n",
    "\n",
    "integer_to_word = dict([(value, key) for (key, value) in word_to_integer.items()])\n",
    "\n",
    "# Demonstrate how to find the word from an integer\n",
    "print(integer_to_word[1])\n",
    "print(integer_to_word[2])\n",
    "\n",
    "# We need to subtract 3 from the indices because 0 is \"padding\", 1 is \"start of sequence\", and 2 is \"unknown\"\n",
    "decoded_review = ' '.join([integer_to_word.get(i - 3, 'UNK') for i in train_data[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFP_XKVRp4_S",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3) Format the data\n",
    "Unfortunately, we cannot just feed unformatted arrays into our neural network. We need to standardize the input. Here, we are going to multi-hot-encode our review which is an array of integers into a 10,000 dimensional vector. We will place 1's in the indices of word-integers that occur in the review, and 0's everywhere else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NG4K-OtdqC0a"
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension), dtype=np.float32)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "train_data = vectorize_sequences(train_data)\n",
    "test_data = vectorize_sequences(test_data)\n",
    "\n",
    "print(train_data.shape) # length is same as before\n",
    "print(train_data[0]) # now, multi-hot encoded\n",
    "\n",
    "# Vectorize the labels as well and reshape from (N, ) to (N, 1)\n",
    "train_labels = np.reshape(np.asarray(train_labels, dtype=np.float32), (len(train_data), 1))\n",
    "test_labels = np.reshape(np.asarray(test_labels, dtype=np.float32), (len(test_data), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeYlLZlrqYjq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 4) Create the model\n",
    "\n",
    "Now that we have vectorized input, we're ready to build our neural network. We're going to use multiple hidden layers, each with 16 hidden units. The more layers and hidden units you have, the more complicated patterns the network can recognize and learn. \n",
    "\n",
    "* The downside is that a large number potentially leads to [overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting) by finding patterns that may only exist in your training not (and not generalize to unseen testing data down the road).\n",
    "\n",
    "Instead of softmax as in the previous notebook, here our last activation will be a sigmoid. This will take the output of the first two layers and \"squish\" it into a number between 0 and 1 (the network's prediction for the review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uVOGq_eirutM"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    " # input shape here is the length of our movie review vector\n",
    "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(10000,)))\n",
    "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIBgYoE1sCmw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 5) Create a validation set\n",
    "\n",
    "We want to test our model on data it hasn't seen before, but before we get our final test accuracy. We'll call this dataset our validation set. In practice, it's used to tune parameters like your learning rate, or the number of layers / units in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CfoGOjesswbb"
   },
   "outputs": [],
   "source": [
    "VAL_SIZE = 10000\n",
    "\n",
    "val_data = train_data[:VAL_SIZE]\n",
    "partial_train_data = train_data[VAL_SIZE:]\n",
    "\n",
    "\n",
    "val_labels = train_labels[:VAL_SIZE]\n",
    "partial_train_labels = train_labels[VAL_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBvKRJIts5U4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 6) Create a tf.data Dataset\n",
    "As before, we're working with a small in-memory dataset, so converting to tf.data isn't essential, just good practice for down the road. See the previous notebook or the `tf.data.Dataset` [API doc](https://www.tensorflow.org/api_docs/python/tf/data) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dz9EDovvtGa6"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "SHUFFLE_SIZE = 1000\n",
    "\n",
    "training_set = tf.data.Dataset.from_tensor_slices((partial_train_data, partial_train_labels))\n",
    "training_set = training_set.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nh9l8Cdfte8g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 7) Training your model\n",
    "Now we are going to train the model. As we go, we'll keep track of the training loss, training accuracy, validation loss, and validation accuracy. \n",
    "\n",
    "Please be patient as this step may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_UVH5tQqtcxc"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "# Store list of metric values for plotting later\n",
    "tr_loss_list = []\n",
    "tr_accuracy_list = []\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for reviews, labels in training_set:\n",
    "    # Calculate training loss and accuracy\n",
    "    tr_loss, tr_accuracy = model.train_on_batch(reviews, labels)\n",
    "  \n",
    "  # Calculate validation loss and accuracy\n",
    "  val_loss, val_accuracy = model.evaluate(val_data, val_labels)\n",
    "\n",
    "  # Add to the lists\n",
    "  tr_loss_list.append(tr_loss)\n",
    "  tr_accuracy_list.append(tr_accuracy)\n",
    "  val_loss_list.append(val_loss)\n",
    "  val_accuracy_list.append(val_accuracy)\n",
    "  \n",
    "  print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n",
    "         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n",
    "         % (epoch + 1, tr_loss, tr_accuracy,\n",
    "         val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNzTBpLBvPPn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 8) Plotting loss and accuracy\n",
    "We are going to use `matplotlib` to plot our training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kcBHF0rKt4Rn"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "\n",
    "# \"bo\" specifies \"blue dot\"\n",
    "plt.plot(epochs, tr_loss_list, 'bo', label='Training loss')\n",
    "# b specifies a \"solid blue line\"\n",
    "plt.plot(epochs, val_loss_list, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sx1Vg_1qvT5D",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()   # Clear plot\n",
    "\n",
    "plt.plot(epochs, tr_accuracy_list, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy_list, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rX_qklYwvak2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 9) Testing your model\n",
    "Now that we have successfully trained our model and our training accuracy has jumped over 95%, we need to test it on an entirely different dataset. We are going to run our model on the test set. The test accuracy is a better evaluation metric for how our model will perform in the real world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Oghmxes4vhs-"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bl7gZC_lvxyD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Congratulations\n",
    "You have successfully trained a model on the IMDB dataset."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "3-imdb.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
